{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+8J+ehkuJMSKC2x1lxAAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DikshantBadawadagi/NLP/blob/main/Pre-Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v560rJovxhCl"
      },
      "outputs": [],
      "source": [
        "!pip install nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\" Hi this is Dikshant's notebook, and youre learning NLP with me.\n",
        "Welcome to my notebook, today ill be teaching youll how to tokenize the data/corpus \"\"\""
      ],
      "metadata": {
        "id": "bvfUCvvUxlKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEGiZG9vx7MG",
        "outputId": "c5e6f904-15b6-45a6-d207-2cdb75ba14f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hi this is Dikshant's notebook, and youre learning NLP with me.\n",
            "Welcome to my notebook, today ill be teaching youll how to tokenize the data/corpus \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3I-co6fx72s",
        "outputId": "830e8547-124c-4d2e-9db8-757c99fa4a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "wZhz2zuXyG0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "513192djyJP0",
        "outputId": "aeda09f1-37d3-4290-cc41-4c7fffc7e725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" Hi this is Dikshant's notebook, and youre learning NLP with me.\",\n",
              " 'Welcome to my notebook, today ill be teaching youll how to tokenize the data/corpus']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "uGoUZzgPyhA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtOofPj1yxTm",
        "outputId": "4388bb85-8824-4560-95ca-0c095d4bbb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'this',\n",
              " 'is',\n",
              " 'Dikshant',\n",
              " \"'s\",\n",
              " 'notebook',\n",
              " ',',\n",
              " 'and',\n",
              " 'youre',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'with',\n",
              " 'me',\n",
              " '.',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'my',\n",
              " 'notebook',\n",
              " ',',\n",
              " 'today',\n",
              " 'ill',\n",
              " 'be',\n",
              " 'teaching',\n",
              " 'youll',\n",
              " 'how',\n",
              " 'to',\n",
              " 'tokenize',\n",
              " 'the',\n",
              " 'data/corpus']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "awH2OakJyyvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWVeTayPzE2U",
        "outputId": "80f60ea3-6c0e-4202-a7cb-0c6e7318fa5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'this',\n",
              " 'is',\n",
              " 'Dikshant',\n",
              " \"'\",\n",
              " 's',\n",
              " 'notebook',\n",
              " ',',\n",
              " 'and',\n",
              " 'youre',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'with',\n",
              " 'me',\n",
              " '.',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'my',\n",
              " 'notebook',\n",
              " ',',\n",
              " 'today',\n",
              " 'ill',\n",
              " 'be',\n",
              " 'teaching',\n",
              " 'youll',\n",
              " 'how',\n",
              " 'to',\n",
              " 'tokenize',\n",
              " 'the',\n",
              " 'data',\n",
              " '/',\n",
              " 'corpus']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "g2cdM15gzHEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "1VKDsMEjzcDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "id": "l3Z-Y1GgzfNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "X1J4o889zjJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "RGMNR9AP2L3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word + \"-->\"+stemming.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JYskNqW2PAP",
        "outputId": "fc05f995-fd6a-4900-dd15-51751ccd5343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi-->hi\n",
            "this-->thi\n",
            "is-->is\n",
            "Dikshant-->dikshant\n",
            "'s-->'s\n",
            "notebook-->notebook\n",
            ",-->,\n",
            "and-->and\n",
            "youre-->your\n",
            "learning-->learn\n",
            "NLP-->nlp\n",
            "with-->with\n",
            "me.-->me.\n",
            "Welcome-->welcom\n",
            "to-->to\n",
            "my-->my\n",
            "notebook-->notebook\n",
            ",-->,\n",
            "today-->today\n",
            "ill-->ill\n",
            "be-->be\n",
            "teaching-->teach\n",
            "youll-->youll\n",
            "how-->how\n",
            "to-->to\n",
            "tokenize-->token\n",
            "the-->the\n",
            "data/corpus-->data/corpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "2Qm6eElv2bKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "CFHZKQfE4n2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"-->\"+snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKW8n-fi4srZ",
        "outputId": "278324f9-5be4-4ff7-a265-6d6dca62cd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi-->hi\n",
            "this-->this\n",
            "is-->is\n",
            "Dikshant-->dikshant\n",
            "'s-->'s\n",
            "notebook-->notebook\n",
            ",-->,\n",
            "and-->and\n",
            "youre-->your\n",
            "learning-->learn\n",
            "NLP-->nlp\n",
            "with-->with\n",
            "me.-->me.\n",
            "Welcome-->welcom\n",
            "to-->to\n",
            "my-->my\n",
            "notebook-->notebook\n",
            ",-->,\n",
            "today-->today\n",
            "ill-->ill\n",
            "be-->be\n",
            "teaching-->teach\n",
            "youll-->youll\n",
            "how-->how\n",
            "to-->to\n",
            "tokenize-->token\n",
            "the-->the\n",
            "data/corpus-->data/corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "jjHiPG6_43Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "848Tvu5j6K3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiDw2n4f6kYq",
        "outputId": "479de940-4542-493a-a384-090b522fb3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(lemmatizer.lemmatize(word,pos ='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPKLbnXy6NVq",
        "outputId": "3b1c413a-1848-42e0-9ac2-45c6651cc800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n",
            "this\n",
            "be\n",
            "Dikshant\n",
            "'s\n",
            "notebook\n",
            ",\n",
            "and\n",
            "youre\n",
            "learn\n",
            "NLP\n",
            "with\n",
            "me.\n",
            "Welcome\n",
            "to\n",
            "my\n",
            "notebook\n",
            ",\n",
            "today\n",
            "ill\n",
            "be\n",
            "teach\n",
            "youll\n",
            "how\n",
            "to\n",
            "tokenize\n",
            "the\n",
            "data/corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85PD5fqW6S-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}